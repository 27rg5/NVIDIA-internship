{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-3N1h8mZWLba"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "#import tensorflow.compat.v1 as tf\n",
    "#tf.disable_v2_behavior()\n",
    "#tf.enable_eager_execution()\n",
    "#tf.compat.v1.enable_eager_execution()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import time\n",
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 643
    },
    "id": "NctLvcZsWPek",
    "outputId": "f1684ed7-0f5e-4e1d-efb9-72cb97b14e1a"
   },
   "outputs": [],
   "source": [
    "lms = []\n",
    "count = 0\n",
    "\n",
    "basepath = Path('/content/data/')\n",
    "files_in_basepath = basepath.iterdir()\n",
    "for item in files_in_basepath:\n",
    "    files_in_subdir = item.iterdir()\n",
    "    for item1 in files_in_subdir:\n",
    "        files_in_subdir1 = item1.iterdir()\n",
    "        for data in files_in_subdir1:\n",
    "            loc = str(data)\n",
    "            img = np.load(data)\n",
    "            #img = cv2.imread(loc)\n",
    "            img = cv2.resize(img, dsize=(64, 1000))\n",
    "            if(count == 0):\n",
    "                plt.imshow(img)\n",
    "            count = count + 1\n",
    "            img = (img-127.5)/127.5\n",
    "            lms.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E-nt2UeVWdfZ",
    "outputId": "6f45f32b-df8c-4952-a080-feae14201952"
   },
   "outputs": [],
   "source": [
    "print(np.shape(lms))\n",
    "print(np.shape(lms[6]))\n",
    "lms_array = np.asarray(lms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tp3MZSmNWdiE",
    "outputId": "5682b983-a654-4238-de1b-dfc8377c6d70"
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "faces = []\n",
    "basepath = Path('/content/faces_data/')\n",
    "files_in_basepath = basepath.iterdir()\n",
    "for item in files_in_basepath:\n",
    "    count = 0\n",
    "    files_in_subdir = item.iterdir()\n",
    "    for data in files_in_subdir:\n",
    "        if(count == 0):\n",
    "            faces.append(cv2.imread(str(data)))\n",
    "        count = count+1\n",
    "print(np.shape(faces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQvAKsT4Wdmb"
   },
   "outputs": [],
   "source": [
    "faces_data = np.zeros(shape = (1786, 128, 128, 3))\n",
    "count = -1\n",
    "j = -1\n",
    "basepath = Path('/content/data/')\n",
    "files_in_basepath = basepath.iterdir()\n",
    "for item in files_in_basepath:\n",
    "    count = count + 1\n",
    "    files_in_subdir = item.iterdir()\n",
    "    for item1 in files_in_subdir:\n",
    "        files_in_subdir1 = item1.iterdir()\n",
    "        for data in files_in_subdir1:\n",
    "            j = j + 1\n",
    "            faces_data[j] = faces[count]\n",
    "faces_data = (faces_data - 127.5)/127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viIL_WchWdq7"
   },
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(lms_array, faces_data, test_size=0.15, random_state=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OVrgIDMdWdwt",
    "outputId": "8d307b39-4b51-4ef5-c9f8-2b6190443d49"
   },
   "outputs": [],
   "source": [
    "print(np.shape(trainX))\n",
    "#print(np.shape(valX))\n",
    "print(np.shape(testX))\n",
    "print(np.shape(trainY))\n",
    "#print(np.shape(valY))\n",
    "print(np.shape(testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tFGAM7EWd2O"
   },
   "outputs": [],
   "source": [
    "def generator():\n",
    "#inshape = (64, 1000, 64)\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu', input_shape=(1000,64), strides = 2))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Conv1D(128, kernel_size=3, activation='relu', strides = 2))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu', strides = 2))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.AveragePooling1D(pool_size = 2))\n",
    "    model.add(tf.keras.layers.AveragePooling1D(pool_size = 2))\n",
    "    model.add(tf.keras.layers.AveragePooling1D(pool_size = 2))\n",
    "    model.add(tf.keras.layers.AveragePooling1D(pool_size = 2))\n",
    "    model.add(tf.keras.layers.AveragePooling1D(pool_size = 2))\n",
    "    model.add(tf.keras.layers.AveragePooling1D(pool_size = 2))\n",
    "\n",
    "    model.add(tf.keras.layers.Reshape((1, 1, 64)))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2DTranspose(256, (4, 4), strides=(1, 1)))\n",
    "    model.add(tf.keras.layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding = 'same', activation='relu')) \n",
    "    model.add(tf.keras.layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding = 'same', activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2DTranspose(16, (3, 3), strides=(2, 2), padding = 'same', activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2DTranspose(8, (3, 3), strides=(2, 2), padding = 'same', activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2DTranspose(3, (3, 3), strides=(2, 2), padding = 'same', activation='relu'))  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HPfXQGviW0as",
    "outputId": "2d7900d2-6f9b-44cb-cf07-8f9274641161"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 499, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 499, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 249, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 249, 128)          512       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 124, 64)           24640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 124, 64)           256       \n",
      "_________________________________________________________________\n",
      "average_pooling1d (AveragePo (None, 62, 64)            0         \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 31, 64)            0         \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 15, 64)            0         \n",
      "_________________________________________________________________\n",
      "average_pooling1d_3 (Average (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "average_pooling1d_4 (Average (None, 3, 64)             0         \n",
      "_________________________________________________________________\n",
      "average_pooling1d_5 (Average (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 4, 4, 256)         262400    \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 8, 8, 128)         295040    \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 16, 16, 64)        73792     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 32, 32, 16)        9232      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 64, 64, 8)         1160      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 128, 128, 3)       219       \n",
      "=================================================================\n",
      "Total params: 704,563\n",
      "Trainable params: 704,051\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = generator()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nAQRDeZjW0dQ"
   },
   "outputs": [],
   "source": [
    "def discriminator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(64, (4, 4), strides=(1, 1), input_shape=[128, 128, 3]))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(128, (3, 3), strides=(2, 2)))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(256, (3, 3), strides=(2, 2)))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DWudg5ImW0fj",
    "outputId": "3f122036-b6ee-4e42-bb35-1451c43afdd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 125, 125, 64)      3136      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 125, 125, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 62, 62, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 62, 62, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 30, 256)       295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 30, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 230400)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 230401    \n",
      "=================================================================\n",
      "Total params: 602,561\n",
      "Trainable params: 602,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TyDf0RWW0iE"
   },
   "outputs": [],
   "source": [
    "d_loss = []\n",
    "g_loss = []\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def D_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    d_loss.append(total_loss)\n",
    "    return total_loss\n",
    "  \n",
    "def G_loss(fake_output):\n",
    "    loss =  cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    g_loss.append(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lWkyDd6lW0kk"
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(0.0007)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(0.0007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PDLiQZUqW9jU"
   },
   "outputs": [],
   "source": [
    "def train_step(images1, images2):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(images1, training=True)\n",
    "        real_output = discriminator(images2, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = G_loss(fake_output)\n",
    "        disc_loss = D_loss(real_output, fake_output)\n",
    "\n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-bUoDnzW9l7"
   },
   "outputs": [],
   "source": [
    "def sv_img(model, epoch, test_input):\n",
    "    predictions = model(test_input, training=False)\n",
    "    predictions = (predictions*127.5)+127.5\n",
    "    predictions = predictions.numpy()\n",
    "    plt.imshow(predictions[random.randrange(0, 267)].astype('uint8'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IIAgC0CeW9oh"
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "def train_GAN(dataset1, dataset2, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for image_batch1 in dataset1:\n",
    "            for image_batch2 in dataset2:\n",
    "                train_step(image_batch1, image_batch2)\n",
    "                break\n",
    "        print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "        #if(epoch%10 == 0):\n",
    "        sv_img(generator, epoch + 1, testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1qqxVMGW9rI"
   },
   "outputs": [],
   "source": [
    "trainX = tf.convert_to_tensor(trainX , dtype=tf.float32)\n",
    "#trainY = tf.convert_to_tensor(trainY , dtype=tf.float32)\n",
    "testX = tf.convert_to_tensor(testX , dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CtShI6l5W9tw",
    "outputId": "3a677612-2d1b-49e2-fad0-5d88434cedaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1000, 64)\n"
     ]
    }
   ],
   "source": [
    "n = 64\n",
    "trainX = [trainX[i:i + n] for i in range(0, trainX.shape[0], n)]\n",
    "print(np.shape(trainX[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l_mmdj5nXK6r",
    "outputId": "a8875c50-1ad7-422d-9b51-1efa4988f6b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "n = 64\n",
    "trainY = [trainY[i:i + n] for i in range(0, trainY.shape[0], n)]\n",
    "print(np.shape(trainY[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "M-xzzLMCXK-3",
    "outputId": "3a6da11d-af67-447b-e359-e06b3adce7ae"
   },
   "outputs": [],
   "source": [
    "final_model = train_GAN(trainX, trainY, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82XmQ2mktB9m"
   },
   "outputs": [],
   "source": [
    "#final_model.save('/content/GAN.h5')\n",
    "#files.download('/content/GAN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zyk2vVq3kYFH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled18.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
